{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d608997-eeff-4c7e-b205-da7450bb4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import csv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd().parent.parent.parent.parent\n",
    "DATA_DIR = BASE_DIR / \"oc_mini\"\n",
    "\n",
    "# Add gnn package to path (parent directory)\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxv7w87yxor",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "You can easily swap the transformer model and data source by modifying these variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25yowmg3we3",
   "metadata": {},
   "source": [
    "# GNN Baseline 1: Train-only Graph with Frozen Transformer\n",
    "\n",
    "**Experiment Goal**: Demonstrate that test nodes receive NO benefit from GNN when trained separately on a train-only graph.\n",
    "\n",
    "**Setup**:\n",
    "- 90% train nodes, 10% test nodes  \n",
    "- Create train-only graph (edges only between train nodes, indices 0..N-1)\n",
    "- Compute embeddings separately:\n",
    "  - **Train nodes**: Transformer → GNN (graph-enhanced)\n",
    "  - **Test nodes**: Transformer ONLY (no GNN applied)\n",
    "- Transformer (SciBERT) is frozen\n",
    "- Only GNN layers are trainable\n",
    "- Input: Title + Abstract concatenated\n",
    "\n",
    "**Expected Result**: Test nodes will have pure transformer embeddings (no graph enhancement), demonstrating the degradation when train/test graphs are separated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcfc18cd-cb4e-44ca-ac45-95ec3dbf24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GNN modules\n",
    "from model import TransformerGNN\n",
    "from graph_utils import (\n",
    "    create_induced_subgraph,\n",
    "    create_train_only_graph,\n",
    "    analyze_graph_statistics,\n",
    "    get_node_texts\n",
    ")\n",
    "from split_utils import create_node_based_split\n",
    "\n",
    "# Data paths\n",
    "edgelist_path = DATA_DIR / \"network\" / \"oc_mini_edgelist.csv\"\n",
    "metadata_path = DATA_DIR / \"metadata\" / \"oc_mini_node_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8f37c2-ce7b-48ae-babd-21b6f06976c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded: 14442 entries\n",
      "\n",
      "Train nodes: 12998 (90.0%)\n",
      "Test nodes: 1444 (10.0%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>10.1101/2021.05.10.443415</td>\n",
       "      <td>Improved protein contact prediction using dime...</td>\n",
       "      <td>AbstractDeep residual learning has shown great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163</td>\n",
       "      <td>10.1101/2021.05.07.443114</td>\n",
       "      <td>Following the Trail of One Million Genomes: Fo...</td>\n",
       "      <td>AbstractSevere acute respiratory syndrome coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>10.1101/2021.05.11.443555</td>\n",
       "      <td>Mechanism of molnupiravir-induced SARS-CoV-2 m...</td>\n",
       "      <td>Molnupiravir is an orally available antiviral ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>941</td>\n",
       "      <td>10.3390/ijms20020449</td>\n",
       "      <td>Bactericidal and Cytotoxic Properties of Silve...</td>\n",
       "      <td>Silver nanoparticles (AgNPs) can be synthesize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1141</td>\n",
       "      <td>10.3390/ijms20040865</td>\n",
       "      <td>Silver Nanoparticles: Synthesis and Applicatio...</td>\n",
       "      <td>Over the past few decades, metal nanoparticles...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                        doi  \\\n",
       "0   128  10.1101/2021.05.10.443415   \n",
       "1   163  10.1101/2021.05.07.443114   \n",
       "2   200  10.1101/2021.05.11.443555   \n",
       "3   941       10.3390/ijms20020449   \n",
       "4  1141       10.3390/ijms20040865   \n",
       "\n",
       "                                               title  \\\n",
       "0  Improved protein contact prediction using dime...   \n",
       "1  Following the Trail of One Million Genomes: Fo...   \n",
       "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...   \n",
       "3  Bactericidal and Cytotoxic Properties of Silve...   \n",
       "4  Silver Nanoparticles: Synthesis and Applicatio...   \n",
       "\n",
       "                                            abstract  \n",
       "0  AbstractDeep residual learning has shown great...  \n",
       "1  AbstractSevere acute respiratory syndrome coro...  \n",
       "2  Molnupiravir is an orally available antiviral ...  \n",
       "3  Silver nanoparticles (AgNPs) can be synthesize...  \n",
       "4  Over the past few decades, metal nanoparticles...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metadata\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "print(f\"Metadata loaded: {len(metadata_df)} entries\")\n",
    "\n",
    "# Create train/test split (90/10 split)\n",
    "all_node_ids = [str(node_id) for node_id in metadata_df['id'].values]\n",
    "train_nodes, test_nodes = create_node_based_split(all_node_ids, test_ratio=0.1, seed=42)\n",
    "\n",
    "print(f\"\\nTrain nodes: {len(train_nodes)} ({len(train_nodes)/len(all_node_ids)*100:.1f}%)\")\n",
    "print(f\"Test nodes: {len(test_nodes)} ({len(test_nodes)/len(all_node_ids)*100:.1f}%)\")\n",
    "\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7d61dc-1313-49e1-9f62-55eb964d99bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edgelist from /home/vikramr2/oc_mini/network/oc_mini_edgelist.csv...\n",
      "  Full graph: 111873 edges\n",
      "\n",
      "Filtering to edges between 12998 training nodes...\n",
      "  Filtered edges: 91586\n",
      "\n",
      "Node mapping (train only):\n",
      "  Train nodes: 12998\n",
      "  Index range: 0 to 12997\n",
      "\n",
      "Final edge_index shape: torch.Size([2, 183172])\n",
      "  Directed edges: 183172\n",
      "\n",
      "Train-only graph created:\n",
      "  Train nodes: 12998\n",
      "  Edges: 183172\n"
     ]
    }
   ],
   "source": [
    "# Create train-only graph for GNN\n",
    "# This graph contains ONLY training nodes with remapped indices 0..N-1\n",
    "# Test nodes are not included at all\n",
    "\n",
    "train_edge_index, train_node_to_idx, train_idx_to_node = create_train_only_graph(\n",
    "    edgelist_path,\n",
    "    train_nodes\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain-only graph created:\")\n",
    "print(f\"  Train nodes: {len(train_node_to_idx)}\")\n",
    "print(f\"  Edges: {train_edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90zaec92km",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Transformer weights frozen\n",
      "✓ Created 2-layer GCN model\n",
      "  Total GNN parameters: 1,181,184\n",
      "\n",
      "Model Summary:\n",
      "  Transformer: allenai/scibert_scivocab_uncased\n",
      "  Total parameters: 111,102,720\n",
      "  Trainable (GNN only): 1,184,256\n",
      "  Frozen (Transformer): 109,918,464\n"
     ]
    }
   ],
   "source": [
    "# Initialize TransformerGNN model with SciBERT\n",
    "# Key: Transformer is FROZEN, only GNN layers are trainable\n",
    "\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = TransformerGNN(\n",
    "    model_name=model_name,\n",
    "    gnn_type='gcn',           # Use GCN layers\n",
    "    hidden_dim=768,           # Match SciBERT output\n",
    "    num_gnn_layers=2,         # 2 GNN layers\n",
    "    dropout=0.1,\n",
    "    pooling='cls',\n",
    "    freeze_transformer=True   # IMPORTANT: Freeze transformer!\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "print(f\"  Transformer: {model_name}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable (GNN only): {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Frozen (Transformer): {sum(p.numel() for p in model.parameters() if not p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8x5l0g7une",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BASELINE: Separate Train (GNN) vs Test (Transformer-only)\n",
      "======================================================================\n",
      "\n",
      "1. Computing transformer embeddings for TRAIN nodes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9402cfe714094c4faf83d57ba76dbb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train transformer:   0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train transformer embeddings: torch.Size([12998, 768])\n",
      "\n",
      "2. Applying GNN to TRAIN nodes only...\n",
      "   Train GNN embeddings: torch.Size([12998, 768])\n",
      "\n",
      "3. Computing transformer-only embeddings for TEST nodes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78782676e025461c983ac76b030a98e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test transformer:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test transformer embeddings: torch.Size([1444, 768])\n",
      "\n",
      "4. Creating combined embedding dictionary...\n",
      "   Total embeddings: 14442\n",
      "   Train (GNN-enhanced): 12998\n",
      "   Test (Transformer-only): 1444\n",
      "\n",
      "======================================================================\n",
      "BASELINE RESULT:\n",
      "- Train nodes: Transformer → GNN (graph-enhanced embeddings)\n",
      "- Test nodes: Transformer ONLY (no graph information)\n",
      "\n",
      "This demonstrates that test nodes get NO benefit from the GNN\n",
      "because they have no edges in the training graph.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# BASELINE DEMONSTRATION - CORRECTED APPROACH\n",
    "# 1. Compute GNN embeddings ONLY on train nodes (using train-only graph)\n",
    "# 2. Compute transformer-only embeddings for test nodes\n",
    "# This way test nodes truly get transformer-only, demonstrating the degradation\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE: Separate Train (GNN) vs Test (Transformer-only)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Step 1: Get transformer embeddings for TRAIN nodes only\n",
    "print(\"\\n1. Computing transformer embeddings for TRAIN nodes...\")\n",
    "# Sort train nodes by their index in the train graph (0..N-1)\n",
    "train_node_list = [train_idx_to_node[i] for i in range(len(train_idx_to_node))]\n",
    "train_texts = get_node_texts(train_node_list, metadata_df)\n",
    "\n",
    "train_transformer_embs = []\n",
    "batch_size = 32\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(train_texts), batch_size), desc=\"Train transformer\"):\n",
    "        batch_texts = train_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        batch_embs = model.encode_text(inputs['input_ids'], inputs['attention_mask'])\n",
    "        train_transformer_embs.append(batch_embs)\n",
    "\n",
    "train_transformer_embs = torch.cat(train_transformer_embs, dim=0)\n",
    "print(f\"   Train transformer embeddings: {train_transformer_embs.shape}\")\n",
    "\n",
    "# Step 2: Apply GNN ONLY to train nodes with train-only edge index\n",
    "print(\"\\n2. Applying GNN to TRAIN nodes only...\")\n",
    "with torch.no_grad():\n",
    "    train_gnn_embs = model(train_transformer_embs, train_edge_index.to(device))\n",
    "print(f\"   Train GNN embeddings: {train_gnn_embs.shape}\")\n",
    "\n",
    "# Step 3: Get transformer embeddings for TEST nodes (no GNN!)\n",
    "print(\"\\n3. Computing transformer-only embeddings for TEST nodes...\")\n",
    "test_texts = get_node_texts(test_nodes, metadata_df)\n",
    "\n",
    "test_transformer_embs = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_texts), batch_size), desc=\"Test transformer\"):\n",
    "        batch_texts = test_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        batch_embs = model.encode_text(inputs['input_ids'], inputs['attention_mask'])\n",
    "        test_transformer_embs.append(batch_embs)\n",
    "\n",
    "test_transformer_embs = torch.cat(test_transformer_embs, dim=0)\n",
    "print(f\"   Test transformer embeddings: {test_transformer_embs.shape}\")\n",
    "\n",
    "# Step 4: Create combined embedding dictionary\n",
    "print(\"\\n4. Creating combined embedding dictionary...\")\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Add train nodes (with GNN)\n",
    "train_gnn_embs_np = train_gnn_embs.cpu().numpy()\n",
    "for i, node_id in enumerate(train_node_list):\n",
    "    embeddings_dict[node_id] = train_gnn_embs_np[i]\n",
    "\n",
    "# Add test nodes (transformer-only)\n",
    "test_transformer_embs_np = test_transformer_embs.cpu().numpy()\n",
    "for i, node_id in enumerate(test_nodes):\n",
    "    embeddings_dict[node_id] = test_transformer_embs_np[i]\n",
    "\n",
    "print(f\"   Total embeddings: {len(embeddings_dict)}\")\n",
    "print(f\"   Train (GNN-enhanced): {len(train_node_list)}\")\n",
    "print(f\"   Test (Transformer-only): {len(test_nodes)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE RESULT:\")\n",
    "print(\"- Train nodes: Transformer → GNN (graph-enhanced embeddings)\")\n",
    "print(\"- Test nodes: Transformer ONLY (no graph information)\")\n",
    "print(\"\\nThis demonstrates that test nodes get NO benefit from the GNN\")\n",
    "print(\"because they have no edges in the training graph.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d065da-beae-4b77-99cc-d4bd9b339280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
