{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d608997-eeff-4c7e-b205-da7450bb4ba7",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm.auto import tqdm\nimport warnings\nimport json\nimport csv\nwarnings.filterwarnings('ignore')\n\n# Paths\nBASE_DIR = Path.cwd().parent.parent.parent.parent\nDATA_DIR = BASE_DIR / \"oc_mini\"\n\n# Add gnn package to path (parent directory)\nsys.path.insert(0, str(Path.cwd().parent))\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "id": "sxv7w87yxor",
   "source": "## Configuration\n\nYou can easily swap the transformer model and data source by modifying these variables below.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "25yowmg3we3",
   "source": "# GNN Baseline 1: Induced Subgraph with Frozen Transformer\n\n**Experiment Goal**: Demonstrate that when a GNN is trained on an induced subgraph of training nodes (with frozen transformer), test nodes with no edges degrade to transformer-only performance.\n\n**Setup**:\n- 90% train nodes, 10% test nodes\n- Graph contains ONLY edges between training nodes\n- Transformer (SciBERT) is frozen\n- Only GNN layers are trainable\n- Input: Title + Abstract concatenated\n\n**Expected Result**: Test nodes will have identical embeddings from transformer-only vs GNN, since they have no graph connectivity.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc18cd-cb4e-44ca-ac45-95ec3dbf24d6",
   "metadata": {},
   "outputs": [],
   "source": "# Import GNN modules\nfrom model import TransformerGNN\nfrom graph_utils import (\n    create_induced_subgraph,\n    analyze_graph_statistics,\n    get_node_texts\n)\nfrom split_utils import create_node_based_split\n\n# Data paths\nedgelist_path = DATA_DIR / \"network\" / \"oc_mini_edgelist.csv\"\nmetadata_path = DATA_DIR / \"metadata\" / \"oc_mini_node_metadata.csv\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f37c2-ce7b-48ae-babd-21b6f06976c3",
   "metadata": {},
   "outputs": [],
   "source": "# Load metadata\nmetadata_df = pd.read_csv(metadata_path)\nprint(f\"Metadata loaded: {len(metadata_df)} entries\")\n\n# Create train/test split (90/10 split)\nall_node_ids = [str(node_id) for node_id in metadata_df['id'].values]\ntrain_nodes, test_nodes = create_node_based_split(all_node_ids, test_ratio=0.1, seed=42)\n\nprint(f\"\\nTrain nodes: {len(train_nodes)} ({len(train_nodes)/len(all_node_ids)*100:.1f}%)\")\nprint(f\"Test nodes: {len(test_nodes)} ({len(test_nodes)/len(all_node_ids)*100:.1f}%)\")\n\nmetadata_df.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d61dc-1313-49e1-9f62-55eb964d99bd",
   "metadata": {},
   "outputs": [],
   "source": "# Create induced subgraph - KEY STEP FOR BASELINE\n# This graph will ONLY contain edges between training nodes\n# Test nodes will have NO edges (degree = 0)\n\nedge_index, node_to_idx, idx_to_node = create_induced_subgraph(\n    edgelist_path,\n    train_nodes,\n    metadata_df\n)\n\nprint(f\"\\nInduced subgraph created:\")\nprint(f\"  Nodes in mapping: {len(node_to_idx)}\")\nprint(f\"  Edges: {edge_index.shape[1]}\")\n\n# Analyze statistics\nanalyze_graph_statistics(edge_index, train_nodes, node_to_idx, metadata_df)"
  },
  {
   "cell_type": "code",
   "id": "f90zaec92km",
   "source": "# Initialize TransformerGNN model with SciBERT\n# Key: Transformer is FROZEN, only GNN layers are trainable\n\nmodel_name = 'allenai/scibert_scivocab_uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel = TransformerGNN(\n    model_name=model_name,\n    gnn_type='gcn',           # Use GCN layers\n    hidden_dim=768,           # Match SciBERT output\n    num_gnn_layers=2,         # 2 GNN layers\n    dropout=0.1,\n    pooling='cls',\n    freeze_transformer=True   # IMPORTANT: Freeze transformer!\n).to(device)\n\nprint(\"\\nModel Summary:\")\nprint(f\"  Transformer: {model_name}\")\nprint(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"  Trainable (GNN only): {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\"  Frozen (Transformer): {sum(p.numel() for p in model.parameters() if not p.requires_grad):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8x5l0g7une",
   "source": "# BASELINE DEMONSTRATION\n# Compare transformer-only vs GNN embeddings for test nodes\n# NOTE: get_node_texts() concatenates title + abstract for each node\n\nprint(\"=\"*70)\nprint(\"BASELINE DEMONSTRATION: GNN Degradation for Test Nodes\")\nprint(\"=\"*70)\n\n# Sample a few test nodes\nimport random\nsample_test_nodes = random.sample(test_nodes, min(5, len(test_nodes)))\n\nmodel.eval()\nwith torch.no_grad():\n    for node_id in sample_test_nodes:\n        # 1. Get transformer-only embedding\n        # get_node_texts concatenates title + abstract\n        text = get_node_texts([node_id], metadata_df)[0]\n        inputs = tokenizer(\n            [text],\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\"\n        ).to(device)\n        \n        transformer_emb = model.encode_text(inputs['input_ids'], inputs['attention_mask'])\n        transformer_emb = transformer_emb.cpu().numpy()[0]\n        \n        # 2. Get GNN embedding (full pipeline)\n        # First get all transformer embeddings (title + abstract for each)\n        all_node_ids = sorted(node_to_idx.keys(), key=lambda x: node_to_idx[x])\n        all_texts = get_node_texts(all_node_ids, metadata_df)\n        \n        # Encode all texts in batches\n        all_embs = []\n        batch_size = 32\n        for i in range(0, len(all_texts), batch_size):\n            batch_texts = all_texts[i:i+batch_size]\n            batch_inputs = tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=512,\n                return_tensors=\"pt\"\n            ).to(device)\n            batch_embs = model.encode_text(batch_inputs['input_ids'], batch_inputs['attention_mask'])\n            all_embs.append(batch_embs)\n        \n        x = torch.cat(all_embs, dim=0)\n        \n        # Apply GNN\n        gnn_embs = model(x, edge_index.to(device))\n        \n        # Get embedding for this test node\n        node_idx = node_to_idx[node_id]\n        gnn_emb = gnn_embs[node_idx].cpu().numpy()\n        \n        # 3. Compare\n        cosine_sim = (transformer_emb * gnn_emb).sum()\n        l2_diff = np.sqrt(((transformer_emb - gnn_emb) ** 2).sum())\n        \n        print(f\"\\nTest Node {node_id}:\")\n        print(f\"  Cosine similarity: {cosine_sim:.6f} (1.0 = identical)\")\n        print(f\"  L2 difference: {l2_diff:.8f} (0.0 = identical)\")\n        \n        if cosine_sim > 0.999:\n            print(f\"  âœ“ GNN = Transformer (as expected for nodes with no edges)\")\n        else:\n            print(f\"  ! Embeddings differ (unexpected)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"KEY INSIGHT:\")\nprint(\"Test nodes have NO edges in the induced subgraph,\")\nprint(\"so GNN provides NO benefit - it's just the transformer!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}