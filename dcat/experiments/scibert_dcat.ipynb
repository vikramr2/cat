{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019626c3-8719-4ed3-b8ce-42fdcf49d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd().parent.parent\n",
    "DATA_DIR = BASE_DIR / \"oc_mini\"\n",
    "\n",
    "# Add hcat package to path\n",
    "sys.path.insert(0, str(BASE_DIR / \"cat\" / \"dcat\"))\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b2d67-5626-49ad-be15-90d22718dce1",
   "metadata": {},
   "outputs": [],
   "source": "# Load clustering and metadata\nmetadata_path = DATA_DIR / \"metadata\" / \"oc_mini_node_metadata.csv\"\nclustering_path = DATA_DIR / \"clustering\" / \"disjoint\" / \"oc_mini_clusters_0.001.csv\"\n\nmetadata_df = pd.read_csv(metadata_path)\nclustering_df = pd.read_csv(clustering_path)\n\nprint(f\"Metadata loaded: {len(metadata_df)} entries\")\nprint(f\"Clustering loaded: {len(clustering_df)} entries\")\n\n# IMPORTANT: Create test split BEFORE training\n# This ensures validation and test sets use the same nodes\nfrom notebook_utils import create_test_split\n\nall_node_ids = [str(node_id) for node_id in metadata_df['id'].values]\ntest_val_nodes = create_test_split(all_node_ids, test_ratio=0.1, seed=42)\n\nprint(f\"\\nTest/Val set: {len(test_val_nodes)} nodes ({len(test_val_nodes)/len(all_node_ids)*100:.1f}%)\")\nprint(f\"Train set: {len(all_node_ids) - len(test_val_nodes)} nodes\")\n\nmetadata_df.head()"
  },
  {
   "cell_type": "code",
   "id": "l3mwboiaoxc",
   "source": "from train import train_model\n\n# Train model with standard triplet loss\n# Using test_val_nodes for validation to ensure consistency\nfinetuned_model, tokenizer, history = train_model(\n    clustering_csv_path=str(clustering_path),\n    metadata_csv_path=str(metadata_path),\n    output_dir=str(BASE_DIR / \"cat\" / \"models\" / \"finetuned_dcat_triplet\"),\n    model_name='allenai/scibert_scivocab_uncased',\n    device=str(device),\n    batch_size=16,\n    epochs=3,\n    lr=1e-5,\n    margin=0.5,              # Standard triplet margin\n    samples_per_node=3,      # ~43K triplets for 14K nodes\n    pooling='cls',\n    loss_type='triplet',     # Standard triplet loss (not adaptive)\n    val_nodes=test_val_nodes  # Use same nodes for validation as we'll use for testing\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5fsqdq7rcxr",
   "source": "# Plot training curves\nplt.figure(figsize=(10, 5))\nplt.plot(history['train_loss'], marker='o', label='Train Loss', linewidth=2)\nplt.plot(history['val_loss'], marker='s', label='Val Loss', linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Disjoint Clustering Triplet Loss Training', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\\nFinal train loss: {history['train_loss'][-1]:.4f}\")\nprint(f\"Final val loss: {history['val_loss'][-1]:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fwlg2a5u26u",
   "source": "from notebook_utils import compute_embeddings\n\n# Load the BEST saved model (not the final epoch model)\noutput_dir = BASE_DIR / \"cat\" / \"models\" / \"finetuned_dcat_triplet\"\n\nprint(\"Loading best saved model...\")\nbest_tokenizer = AutoTokenizer.from_pretrained(str(output_dir))\nbest_model = AutoModel.from_pretrained(str(output_dir)).to(device)\nbest_model.eval()\n\nprint(f\"✓ Loaded best model from: {output_dir}\")\n\n# Compute embeddings for all nodes using BEST model\nprint(\"\\nComputing embeddings with best model...\")\nembeddings_dict = compute_embeddings(\n    best_model,  # Use BEST saved model\n    best_tokenizer,\n    metadata_df,\n    device,\n    batch_size=32\n)\n\nprint(f\"✓ Computed embeddings for {len(embeddings_dict)} nodes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tmpqc0xmr5e",
   "source": "# Import evaluation functions\nsys.path.insert(0, str(BASE_DIR / \"cat\" / \"experiments\"))\n\nfrom network_link_prediction import (\n    evaluate_network_link_prediction,\n    plot_link_prediction_results\n)\n\nedgelist_path = DATA_DIR / \"network\" / \"oc_mini_edgelist.csv\"\n\n# Evaluate fine-tuned model using the SAME nodes as validation\nprint(f\"Evaluating on test set: {len(test_val_nodes)} nodes\")\nresults = evaluate_network_link_prediction(\n    edgelist_path=str(edgelist_path),\n    embeddings_dict=embeddings_dict,\n    test_nodes=test_val_nodes,  # Same as validation nodes\n    k_values=[5, 10, 20, 50, 100],\n    compute_auc=True,\n    num_negative_samples=10\n)\n\n# Visualize\nplot_link_prediction_results(results['topk'], results['auc'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "anbjm3a9vy",
   "source": "# Load baseline SciBERT (not fine-tuned)\nprint(\"Loading baseline model...\")\nbaseline_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n\n# Compute baseline embeddings\nprint(\"Computing baseline embeddings...\")\nbaseline_embeddings = compute_embeddings(\n    baseline_model,\n    tokenizer,\n    metadata_df,\n    device,\n    batch_size=32\n)\n\nprint(f\"✓ Baseline embeddings: {len(baseline_embeddings)} nodes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ygjl2pio2qe",
   "source": "# Evaluate baseline\nprint(\"Evaluating baseline...\")\nbaseline_results = evaluate_network_link_prediction(\n    edgelist_path=str(edgelist_path),\n    embeddings_dict=baseline_embeddings,\n    test_nodes=test_val_nodes,\n    k_values=[5, 10, 20, 50, 100],\n    compute_auc=True,\n    num_negative_samples=10\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "x5oq6ys03c",
   "source": "# Create comparison table\nk_values = [5, 10, 20, 50, 100]\ncomparison_data = []\n\nfor k in k_values:\n    baseline_prec = baseline_results['topk']['summary'][k]['precision@k']\n    finetuned_prec = results['topk']['summary'][k]['precision@k']\n\n    comparison_data.append({\n        'K': k,\n        'Baseline Precision': baseline_prec,\n        'Fine-tuned Precision': finetuned_prec,\n        'Improvement': finetuned_prec - baseline_prec,\n        'Improvement %': ((finetuned_prec - baseline_prec) / baseline_prec) * 100\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PRECISION@K COMPARISON: Baseline vs Fine-tuned (DCAT Triplet Loss)\")\nprint(\"=\"*80)\nprint(comparison_df.to_string(index=False))\n\n# AUC comparison\nprint(\"\\n\" + \"=\"*80)\nprint(\"AUC METRICS\")\nprint(\"=\"*80)\nprint(f\"Baseline AUC-ROC:    {baseline_results['auc']['auc_roc']:.4f}\")\nprint(f\"Fine-tuned AUC-ROC:  {results['auc']['auc_roc']:.4f}\")\nprint(f\"Improvement:         {results['auc']['auc_roc'] - baseline_results['auc']['auc_roc']:.4f} \" +\n      f\"({((results['auc']['auc_roc'] - baseline_results['auc']['auc_roc'])/baseline_results['auc']['auc_roc'])*100:.1f}%)\")\nprint()\nprint(f\"Baseline AUC-PR:     {baseline_results['auc']['auc_pr']:.4f}\")\nprint(f\"Fine-tuned AUC-PR:   {results['auc']['auc_pr']:.4f}\")\nprint(f\"Improvement:         {results['auc']['auc_pr'] - baseline_results['auc']['auc_pr']:.4f} \" +\n      f\"({((results['auc']['auc_pr'] - baseline_results['auc']['auc_pr'])/baseline_results['auc']['auc_pr'])*100:.1f}%)\")\n\n# Visualize\nplot_link_prediction_results(baseline_results['topk'], baseline_results['auc'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "t9bgto51o1g",
   "source": "from sklearn.metrics import roc_curve\n\n# Side-by-side comparison plot\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# AUC-ROC\nax = axes[0]\n\nfpr, tpr, _ = roc_curve(results['auc']['y_true'], results['auc']['y_scores'])\nbaseline_fpr, baseline_tpr, _ = roc_curve(results['auc']['y_true'], baseline_results['auc']['y_scores'])\n\nax.plot(baseline_fpr, baseline_tpr, linewidth=2, label=f\"Baseline = {baseline_results['auc']['auc_roc']:.3f}\", color='#95a5a6')\nax.plot(fpr, tpr, linewidth=2, label=f\"D-CAT = {results['auc']['auc_roc']:.3f}\", color='#3498db')\nax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate', fontsize=12)\nax.set_title('AUC-ROC', fontsize=13, fontweight='bold')\nax.legend(fontsize=11)\n\n# Hit Rate @ K\nax = axes[1]\nx = np.arange(len(k_values))\nwidth = 0.35\n\nbaseline_prec = [baseline_results['topk']['summary'][k]['hit_rate@k'] for k in k_values]\nfinetuned_prec = [results['topk']['summary'][k]['hit_rate@k'] for k in k_values]\n\nbars1 = ax.bar(x - width/2, baseline_prec, width, label='Baseline', alpha=0.8, color='#95a5a6')\nbars2 = ax.bar(x + width/2, finetuned_prec, width, label='D-CAT', alpha=0.8, color='#3498db')\n\nax.set_xlabel('K', fontsize=12)\nax.set_ylabel('Hits@K', fontsize=12)\nax.set_title('Link Prediction: Hits@K', fontsize=13, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(k_values)\nax.legend(fontsize=11)\nax.grid(True, axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tbge0ggjz7",
   "source": "# Create text dictionary for content evaluation\ntext_dict = {}\nfor _, row in metadata_df.iterrows():\n    node_id = str(row['id'])\n    title = str(row['title']) if pd.notna(row['title']) else ''\n    abstract = str(row['abstract']) if pd.notna(row['abstract']) else ''\n    \n    combined_text = f\"{title} {abstract}\".strip()\n    if combined_text:\n        text_dict[node_id] = combined_text\n\nprint(f\"Created text dictionary with {len(text_dict)} entries\")\n\n# Verify overlap\noverlap = set(embeddings_dict.keys()) & set(text_dict.keys())\nprint(f\"Overlap: {len(overlap)} keys in common\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uwtpphp1au",
   "source": "# Import evaluation functions\nsys.path.insert(0, str(BASE_DIR / \"cat\"))\n\nfrom utils.evaluation.content_eval import evaluate_content_preservation\n\ncontent_results = evaluate_content_preservation(\n    embeddings_dict=embeddings_dict,\n    baseline_embeddings_dict=baseline_embeddings,\n    content_dict=text_dict,\n    test_nodes=test_val_nodes,\n    sample_size=2000,\n    random_state=42\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "i94nfhoowgq",
   "source": "baseline_spearman = content_results['baseline']['spearman_correlation']\nfinetuned_spearman = content_results['finetuned']['spearman_correlation']\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\nx = np.arange(1)\nwidth = 0.35\n\nbaseline_prec = [baseline_spearman]\nfinetuned_prec = [finetuned_spearman]\n\nbars1 = ax.bar(x - width/2, baseline_prec, width, label='Baseline', alpha=0.8, color='#95a5a6')\nbars2 = ax.bar(x + width/2, finetuned_prec, width, label='D-CAT', alpha=0.8, color='#3498db')\n\nax.set_ylabel('Spearman Correlation', fontsize=12)\nax.set_title('Content Preservation', fontsize=13, fontweight='bold')\nplt.tick_params(\n    axis='x',\n    which='both',\n    bottom=False,\n    top=False,\n    labelbottom=False)\nax.legend(fontsize=11)\nax.grid(True, axis='y', alpha=0.3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "stp3q6pgqz9",
   "source": "from utils.evaluation.graph_dist_eval import evaluate_distance_correlation, plot_distance_correlation\n\nedgelist_path = DATA_DIR / \"network\" / \"oc_mini_edgelist.csv\"\n\n# Evaluate fine-tuned model\nfinetuned_distance_evaluation = evaluate_distance_correlation(\n    edgelist_path=edgelist_path,\n    embeddings_dict=embeddings_dict,\n    test_nodes=test_val_nodes,\n    num_samples_per_node=100,\n    max_graph_distance=10,\n    embedding_distance_metric='cosine',\n    sampling_strategy='stratified'\n)\n\nplot_distance_correlation(finetuned_distance_evaluation)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "akkigz905a",
   "source": "# Evaluate baseline model\nbaseline_distance_evaluation = evaluate_distance_correlation(\n    edgelist_path=edgelist_path,\n    embeddings_dict=baseline_embeddings,\n    test_nodes=test_val_nodes,\n    num_samples_per_node=100,\n    max_graph_distance=10,\n    embedding_distance_metric='cosine',\n    sampling_strategy='stratified'\n)\n\nplot_distance_correlation(baseline_distance_evaluation)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "m9ib23s56ut",
   "source": "# Evaluate clustering structure preservation\nfrom cluster_utils import DisjointClustering\nfrom utils.evaluation.clustering_eval import evaluate_clustering_structure\n\n# Load clustering\nclustering = DisjointClustering(clustering_df)\n\n# Evaluate baseline embeddings\nbaseline_cluster_results = evaluate_clustering_structure(\n    clustering=clustering,\n    embeddings_dict=baseline_embeddings,\n    test_nodes=test_val_nodes,\n    distance_sample_size=2000,\n    random_state=42\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7e2xubz91js",
   "source": "# Evaluate fine-tuned embeddings\nfinetuned_cluster_results = evaluate_clustering_structure(\n    clustering=clustering,\n    embeddings_dict=embeddings_dict,\n    test_nodes=test_val_nodes,\n    distance_sample_size=2000,\n    random_state=42\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "p4nboqx25fm",
   "source": "# Save results\nresults_dir = BASE_DIR / \"cat\" / \"dcat\" / \"experiments\" / \"results\"\nresults_dir.mkdir(parents=True, exist_ok=True)\n\nwith open(results_dir / \"content_results.json\", 'w') as f:\n    json.dump(content_results, f, default=lambda o: float(o) if isinstance(o, np.floating) else int(o) if isinstance(o, np.integer) else o)\n\nwith open(results_dir / \"finetuned_cluster_results.json\", 'w') as f:\n    json.dump(finetuned_cluster_results, f, default=lambda o: float(o) if isinstance(o, np.floating) else int(o) if isinstance(o, np.integer) else o)\n\nwith open(results_dir / \"baseline_cluster_results.json\", 'w') as f:\n    json.dump(baseline_cluster_results, f, default=lambda o: float(o) if isinstance(o, np.floating) else int(o) if isinstance(o, np.integer) else o)\n\nwith open(results_dir / \"baseline_distance_results.json\", 'w') as f:\n    json.dump(baseline_distance_evaluation, f, \n              default=lambda o: o.tolist() if isinstance(o, np.ndarray) else float(o) if isinstance(o, np.floating) else int(o) if isinstance(o, np.integer) else o)\n\nwith open(results_dir / \"finetuned_distance_results.json\", 'w') as f:\n    json.dump(finetuned_distance_evaluation, f, \n              default=lambda o: o.tolist() if isinstance(o, np.ndarray) else float(o) if isinstance(o, np.floating) else int(o) if isinstance(o, np.integer) else o)\n\nwith open(results_dir / \"link_prediction_results.json\", 'w') as f:\n    json.dump(results, f,\n              default=lambda o: o.tolist() if isinstance(o, np.ndarray) else float(o) if isinstance(o, np.floating) else int(o) if isinstance(o, np.integer) else o)\n\nwith open(results_dir / \"baseline_link_prediction_results.json\", 'w') as f:\n    json.dump(baseline_results, f,\n              default=lambda o: o.tolist() if isinstance(o, np.ndarray) else float(o) if isinstance(o, np.floating) else int(o) if isinstance(o, np.integer) else o)\n\nprint(\"✓ All results saved to:\", results_dir)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}