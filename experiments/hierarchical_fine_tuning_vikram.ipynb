{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1456339",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS AND DIRECTORY INITIALIZATION\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_DIR = Path.cwd().parent.parent\n",
    "DATA_DIR = BASE_DIR / \"oc_mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7af254",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_embeddings(texts, model, tokenizer, device):\n",
    "    \"\"\"Compute embeddings for a list of texts.\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff5523d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cluster data: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# CLUSTER LOADING\n",
    "\n",
    "cluster_path = DATA_DIR / \"clustering\" / \"hierarchical\" / \"oc_mini_paris.json\"\n",
    "metadata_path = DATA_DIR / \"metadata\" / \"oc_mini_node_metadata.csv\"\n",
    "\n",
    "with open(cluster_path, 'r') as f:\n",
    "    cluster_data = json.load(f)\n",
    "\n",
    "# Sanity Check\n",
    "print(f\"Loaded cluster data: {len(cluster_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8272d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99bb320ff584e15b8a3e8247a5cd357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3483b74b9e91457b948e84c5f070643b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1ce9f7e2304fbd9461c8541428ea31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3865ee478a34412a77cf1d29c28ea67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciBERT model loaded: allenai/scibert_scivocab_uncased\n",
      "\n",
      "Sample sentence: The relationship between quantum mechanics and general relativity remains one of the most important unsolved problems in theoretical physics.\n",
      "Embedding shape: (1, 768)\n",
      "Embedding (first 10 dimensions): [-0.38251862 -0.10934478  0.50328714  0.07526603  0.40628052 -0.21742031\n",
      "  0.8403158  -0.9267022   0.02716607  0.9420572 ]\n"
     ]
    }
   ],
   "source": [
    "# Load SciBERT model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"SciBERT model loaded: {model_name}\")\n",
    "\n",
    "# Embed a sample sentence\n",
    "sample_sentence = \"The relationship between quantum mechanics and general relativity remains one of the most important unsolved problems in theoretical physics.\"\n",
    "\n",
    "embedding = compute_embeddings([sample_sentence], model, tokenizer, device)\n",
    "\n",
    "print(f\"\\nSample sentence: {sample_sentence}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding (first 10 dimensions): {embedding[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7c7054-50bc-4605-835c-c88f772be808",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>10.1101/2021.05.10.443415</td>\n",
       "      <td>Improved protein contact prediction using dime...</td>\n",
       "      <td>AbstractDeep residual learning has shown great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163</td>\n",
       "      <td>10.1101/2021.05.07.443114</td>\n",
       "      <td>Following the Trail of One Million Genomes: Fo...</td>\n",
       "      <td>AbstractSevere acute respiratory syndrome coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>10.1101/2021.05.11.443555</td>\n",
       "      <td>Mechanism of molnupiravir-induced SARS-CoV-2 m...</td>\n",
       "      <td>Molnupiravir is an orally available antiviral ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>941</td>\n",
       "      <td>10.3390/ijms20020449</td>\n",
       "      <td>Bactericidal and Cytotoxic Properties of Silve...</td>\n",
       "      <td>Silver nanoparticles (AgNPs) can be synthesize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1141</td>\n",
       "      <td>10.3390/ijms20040865</td>\n",
       "      <td>Silver Nanoparticles: Synthesis and Applicatio...</td>\n",
       "      <td>Over the past few decades, metal nanoparticles...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                        doi  \\\n",
       "0   128  10.1101/2021.05.10.443415   \n",
       "1   163  10.1101/2021.05.07.443114   \n",
       "2   200  10.1101/2021.05.11.443555   \n",
       "3   941       10.3390/ijms20020449   \n",
       "4  1141       10.3390/ijms20040865   \n",
       "\n",
       "                                               title  \\\n",
       "0  Improved protein contact prediction using dime...   \n",
       "1  Following the Trail of One Million Genomes: Fo...   \n",
       "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...   \n",
       "3  Bactericidal and Cytotoxic Properties of Silve...   \n",
       "4  Silver Nanoparticles: Synthesis and Applicatio...   \n",
       "\n",
       "                                            abstract  \n",
       "0  AbstractDeep residual learning has shown great...  \n",
       "1  AbstractSevere acute respiratory syndrome coro...  \n",
       "2  Molnupiravir is an orally available antiviral ...  \n",
       "3  Silver nanoparticles (AgNPs) can be synthesize...  \n",
       "4  Over the past few decades, metal nanoparticles...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3626e243-916d-4d76-b907-27dda9c6e35e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 1: HIERARCHICAL TREE UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class TreeNode:\n",
    "    \"\"\"Represents a node in the hierarchical tree\"\"\"\n",
    "    id: int\n",
    "    type: str  # 'leaf' or 'cluster'\n",
    "    name: Optional[str] = None\n",
    "    distance: Optional[float] = None\n",
    "    count: int = 1\n",
    "    children: List['TreeNode'] = None\n",
    "    parent: Optional['TreeNode'] = None\n",
    "    depth: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.children is None:\n",
    "            self.children = []\n",
    "\n",
    "\n",
    "class HierarchicalTree:\n",
    "    \"\"\"Parse and navigate hierarchical tree structure\"\"\"\n",
    "\n",
    "    def __init__(self, tree_dict: Dict):\n",
    "        \"\"\"Load tree from dictionary (loaded JSON)\"\"\"\n",
    "        self.root = self._parse_tree(tree_dict['hierarchy'])\n",
    "        self.leaves = {}  # Map leaf name to TreeNode\n",
    "        self.all_nodes = {}  # Map node id to TreeNode\n",
    "        self._build_indices(self.root)\n",
    "        self._compute_depths(self.root, 0)\n",
    "\n",
    "    def _parse_tree(self, node_dict: Dict, parent=None) -> TreeNode:\n",
    "        \"\"\"Recursively parse tree structure\"\"\"\n",
    "        node = TreeNode(\n",
    "            id=node_dict['id'],\n",
    "            type=node_dict['type'],\n",
    "            name=node_dict.get('name'),\n",
    "            distance=node_dict.get('distance'),\n",
    "            count=node_dict.get('count', 1),\n",
    "            parent=parent\n",
    "        )\n",
    "\n",
    "        if 'children' in node_dict:\n",
    "            node.children = [\n",
    "                self._parse_tree(child, parent=node)\n",
    "                for child in node_dict['children']\n",
    "            ]\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _build_indices(self, node: TreeNode):\n",
    "        \"\"\"Build lookup indices for leaves and all nodes\"\"\"\n",
    "        self.all_nodes[node.id] = node\n",
    "\n",
    "        if node.type == 'leaf':\n",
    "            self.leaves[node.name] = node\n",
    "\n",
    "        for child in node.children:\n",
    "            self._build_indices(child)\n",
    "\n",
    "    def _compute_depths(self, node: TreeNode, depth: int):\n",
    "        \"\"\"Compute depth for each node\"\"\"\n",
    "        node.depth = depth\n",
    "        for child in node.children:\n",
    "            self._compute_depths(child, depth + 1)\n",
    "\n",
    "    def find_lca(self, node1: TreeNode, node2: TreeNode) -> TreeNode:\n",
    "        \"\"\"Find lowest common ancestor of two nodes\"\"\"\n",
    "        # Get ancestors of node1\n",
    "        ancestors1 = set()\n",
    "        current = node1\n",
    "        while current is not None:\n",
    "            ancestors1.add(current.id)\n",
    "            current = current.parent\n",
    "\n",
    "        # Find first common ancestor for node2\n",
    "        current = node2\n",
    "        while current is not None:\n",
    "            if current.id in ancestors1:\n",
    "                return current\n",
    "            current = current.parent\n",
    "\n",
    "        return self.root\n",
    "\n",
    "    def tree_distance(self, node1: TreeNode, node2: TreeNode) -> float:\n",
    "        \"\"\"\n",
    "        Compute hierarchical distance between two nodes.\n",
    "        Returns the depth of their lowest common ancestor (LCA).\n",
    "        Lower depth = more distant (LCA near root)\n",
    "        \"\"\"\n",
    "        lca = self.find_lca(node1, node2)\n",
    "        # Invert so higher = more distant\n",
    "        max_depth = max(node1.depth, node2.depth)\n",
    "        return max_depth - lca.depth\n",
    "\n",
    "    def get_leaves_in_subtree(self, node: TreeNode) -> List[TreeNode]:\n",
    "        \"\"\"Get all leaf nodes under a given node\"\"\"\n",
    "        if node.type == 'leaf':\n",
    "            return [node]\n",
    "\n",
    "        leaves = []\n",
    "        for child in node.children:\n",
    "            leaves.extend(self.get_leaves_in_subtree(child))\n",
    "        return leaves\n",
    "\n",
    "    def sample_triplet(self, anchor_name: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Sample positive and negative for an anchor leaf.\n",
    "        Returns (positive_name, negative_name)\n",
    "        \"\"\"\n",
    "        anchor = self.leaves[anchor_name]\n",
    "        all_leaves = [l for l in self.leaves.values() if l.name != anchor_name]\n",
    "\n",
    "        # Compute distances to all other leaves\n",
    "        distances = [(l, self.tree_distance(anchor, l)) for l in all_leaves]\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "\n",
    "        # Positive: closest leaves (smallest distance)\n",
    "        close_threshold = distances[0][1] if distances else 0\n",
    "        positive_candidates = [l for l, d in distances if d <= close_threshold + 1]\n",
    "        positive = random.choice(positive_candidates) if positive_candidates else distances[0][0]\n",
    "\n",
    "        # Negative: distant leaves (largest distance)\n",
    "        far_threshold = distances[-1][1] if distances else 0\n",
    "        negative_candidates = [l for l, d in distances if d >= far_threshold - 1]\n",
    "        negative = random.choice(negative_candidates) if negative_candidates else distances[-1][0]\n",
    "\n",
    "        return positive.name, negative.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d28034-1a83-4b8b-a67f-effd8d3ab8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 28767\n",
      "Total leaves: 14384\n",
      "Max tree depth: 25\n",
      "\n",
      "Sample leaf: 9393161\n",
      "  Depth: 13\n",
      "  Parent ID: 17816\n",
      "\n",
      "Sampled triplet for 9393161:\n",
      "  Positive: 110740\n",
      "  Negative: 9000723\n",
      "\n",
      "Tree distances:\n",
      "  Anchor ↔ Positive: 1\n",
      "  Anchor ↔ Negative: 24\n",
      "  ✓ Negative should be > Positive: True\n"
     ]
    }
   ],
   "source": [
    "# Build the tree\n",
    "tree = HierarchicalTree(cluster_data)\n",
    "\n",
    "# Print basic stats\n",
    "print(f\"Total nodes: {len(tree.all_nodes)}\")\n",
    "print(f\"Total leaves: {len(tree.leaves)}\")\n",
    "print(f\"Max tree depth: {max(node.depth for node in tree.all_nodes.values())}\")\n",
    "\n",
    "# Test with a random leaf\n",
    "sample_leaf_name = list(tree.leaves.keys())[0]\n",
    "sample_leaf = tree.leaves[sample_leaf_name]\n",
    "\n",
    "print(f\"\\nSample leaf: {sample_leaf_name}\")\n",
    "print(f\"  Depth: {sample_leaf.depth}\")\n",
    "print(f\"  Parent ID: {sample_leaf.parent.id if sample_leaf.parent else 'None'}\")\n",
    "\n",
    "# Sample a triplet\n",
    "pos_name, neg_name = tree.sample_triplet(sample_leaf_name)\n",
    "print(f\"\\nSampled triplet for {sample_leaf_name}:\")\n",
    "print(f\"  Positive: {pos_name}\")\n",
    "print(f\"  Negative: {neg_name}\")\n",
    "\n",
    "# Check distances\n",
    "anchor_node = tree.leaves[sample_leaf_name]\n",
    "pos_node = tree.leaves[pos_name]\n",
    "neg_node = tree.leaves[neg_name]\n",
    "\n",
    "pos_dist = tree.tree_distance(anchor_node, pos_node)\n",
    "neg_dist = tree.tree_distance(anchor_node, neg_node)\n",
    "\n",
    "print(f\"\\nTree distances:\")\n",
    "print(f\"  Anchor ↔ Positive: {pos_dist}\")\n",
    "print(f\"  Anchor ↔ Negative: {neg_dist}\")\n",
    "print(f\"  ✓ Negative should be > Positive: {neg_dist > pos_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc738016-7040-4518-8b26-0652191897ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: TRIPLET DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalTripletDataset(Dataset):\n",
    "    \"\"\"Dataset for hierarchical triplet learning\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tree: HierarchicalTree,\n",
    "        metadata_df: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        samples_per_leaf: int = 3\n",
    "    ):\n",
    "        self.tree = tree\n",
    "        self.metadata_df = metadata_df.set_index('id')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Generate triplets\n",
    "        print(f\"Generating triplets for {len(tree.leaves)} leaves...\")\n",
    "        self.triplets = self._generate_triplets(samples_per_leaf)\n",
    "        print(f\"Generated {len(self.triplets)} triplets\")\n",
    "\n",
    "    def _get_text(self, leaf_name: str) -> str:\n",
    "        \"\"\"Get combined title + abstract for a leaf\"\"\"\n",
    "        try:\n",
    "            row = self.metadata_df.loc[int(leaf_name)]\n",
    "            title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "            abstract = str(row['abstract']) if pd.notna(row['abstract']) else \"\"\n",
    "            return f\"{title} {abstract}\".strip()\n",
    "        except (KeyError, ValueError):\n",
    "            return f\"Document {leaf_name}\"\n",
    "\n",
    "    def _generate_triplets(self, samples_per_leaf: int) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"Generate (anchor, positive, negative) triplets\"\"\"\n",
    "        triplets = []\n",
    "        leaf_names = list(self.tree.leaves.keys())\n",
    "\n",
    "        for leaf_name in tqdm(leaf_names, desc=\"Mining triplets\"):\n",
    "            for _ in range(samples_per_leaf):\n",
    "                pos_name, neg_name = self.tree.sample_triplet(leaf_name)\n",
    "\n",
    "                anchor_text = self._get_text(leaf_name)\n",
    "                pos_text = self._get_text(pos_name)\n",
    "                neg_text = self._get_text(neg_name)\n",
    "\n",
    "                triplets.append((anchor_text, pos_text, neg_text))\n",
    "\n",
    "        return triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor, positive, negative = self.triplets[idx]\n",
    "\n",
    "        # Tokenize all three\n",
    "        anchor_encoded = self.tokenizer(\n",
    "            anchor,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        positive_encoded = self.tokenizer(\n",
    "            positive,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        negative_encoded = self.tokenizer(\n",
    "            negative,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_encoded['input_ids'].squeeze(0),\n",
    "            'anchor_attention_mask': anchor_encoded['attention_mask'].squeeze(0),\n",
    "            'positive_input_ids': positive_encoded['input_ids'].squeeze(0),\n",
    "            'positive_attention_mask': positive_encoded['attention_mask'].squeeze(0),\n",
    "            'negative_input_ids': negative_encoded['input_ids'].squeeze(0),\n",
    "            'negative_attention_mask': negative_encoded['attention_mask'].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afc7d9dc-e69b-452d-a710-12c091aa8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Mean pooling with attention mask\"\"\"\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def encode_batch(model, input_ids, attention_mask, pooling='cls'):\n",
    "    \"\"\"Encode a batch of text to embeddings\"\"\"\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    if pooling == 'mean':\n",
    "        embeddings = mean_pooling(outputs, attention_mask)\n",
    "    else:  # cls\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Normalize\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, margin=0.5, pooling='cls'):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    triplet_loss_fn = TripletMarginLoss(margin=margin)\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        anchor_ids = batch['anchor_input_ids'].to(device)\n",
    "        anchor_mask = batch['anchor_attention_mask'].to(device)\n",
    "        pos_ids = batch['positive_input_ids'].to(device)\n",
    "        pos_mask = batch['positive_attention_mask'].to(device)\n",
    "        neg_ids = batch['negative_input_ids'].to(device)\n",
    "        neg_mask = batch['negative_attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        anchor_emb = encode_batch(model, anchor_ids, anchor_mask, pooling)\n",
    "        pos_emb = encode_batch(model, pos_ids, pos_mask, pooling)\n",
    "        neg_emb = encode_batch(model, neg_ids, neg_mask, pooling)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = triplet_loss_fn(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, margin=0.5, pooling='cls'):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    triplet_loss_fn = TripletMarginLoss(margin=margin)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            anchor_ids = batch['anchor_input_ids'].to(device)\n",
    "            anchor_mask = batch['anchor_attention_mask'].to(device)\n",
    "            pos_ids = batch['positive_input_ids'].to(device)\n",
    "            pos_mask = batch['positive_attention_mask'].to(device)\n",
    "            neg_ids = batch['negative_input_ids'].to(device)\n",
    "            neg_mask = batch['negative_attention_mask'].to(device)\n",
    "\n",
    "            anchor_emb = encode_batch(model, anchor_ids, anchor_mask, pooling)\n",
    "            pos_emb = encode_batch(model, pos_ids, pos_mask, pooling)\n",
    "            neg_emb = encode_batch(model, neg_ids, neg_mask, pooling)\n",
    "\n",
    "            loss = triplet_loss_fn(anchor_emb, pos_emb, neg_emb)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a8a5aef-844b-40fe-95da-c2ba31a81580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def train_hierarchical_triplet_loss(\n",
    "    tree_data,\n",
    "    metadata_df,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    "    margin=0.5,\n",
    "    samples_per_leaf=3,\n",
    "    pooling='cls',\n",
    "    train_split=0.9\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training function\n",
    "\n",
    "    Args:\n",
    "        tree_data: Loaded JSON dict from cluster file\n",
    "        metadata_df: DataFrame with id, title, abstract\n",
    "        model: Pretrained transformer model\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        device: torch device\n",
    "        batch_size: Training batch size\n",
    "        epochs: Number of epochs\n",
    "        lr: Learning rate\n",
    "        margin: Triplet loss margin\n",
    "        samples_per_leaf: Triplets to generate per leaf\n",
    "        pooling: 'cls' or 'mean'\n",
    "        train_split: Train/validation split ratio\n",
    "\n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Build hierarchical tree\n",
    "    print(\"Building hierarchical tree...\")\n",
    "    tree = HierarchicalTree(tree_data)\n",
    "    print(f\"Tree: {len(tree.leaves)} leaves, {len(tree.all_nodes)} total nodes\")\n",
    "\n",
    "    # Create dataset\n",
    "    print(\"\\nCreating dataset...\")\n",
    "    dataset = HierarchicalTripletDataset(\n",
    "        tree=tree,\n",
    "        metadata_df=metadata_df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        samples_per_leaf=samples_per_leaf\n",
    "    )\n",
    "\n",
    "    # Split train/val\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0  # Set to 0 for Jupyter\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(train_dataset)} triplets, Val: {len(val_dataset)} triplets\")\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print('='*60)\n",
    "\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, device, margin=margin, pooling=pooling\n",
    "        )\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, device, margin=margin, pooling=pooling)\n",
    "        print(f\"Val loss: {val_loss:.4f}\")\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"✓ New best validation loss!\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training complete! Best val loss: {best_val_loss:.4f}\")\n",
    "    print('='*60)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f3250a-8016-4cc5-b2e4-114a63bff3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building hierarchical tree...\n",
      "Tree: 14384 leaves, 28767 total nodes\n",
      "\n",
      "Creating dataset...\n",
      "Generating triplets for 14384 leaves...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38d0700c116454fbbf8f8dada5e20a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mining triplets:   0%|          | 0/14384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 43152 triplets\n",
      "Train: 38836 triplets, Val: 4316 triplets\n",
      "\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ae52a9995b42869c10cb2bfbb57d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ccf6157f364b20870efe4c1f609507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0008\n",
      "✓ New best validation loss!\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3180ece82b44480db4fd600e761a7ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f06e12930164fa2875d63b2a77a8d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0013\n",
      "\n",
      "============================================================\n",
      "Epoch 3/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e655f50205442daf33aa3b7947fbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabb85c90db245e7a924af7565485f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0008\n",
      "✓ New best validation loss!\n",
      "\n",
      "============================================================\n",
      "Training complete! Best val loss: 0.0008\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# Train the model with hierarchical triplet loss\n",
    "finetuned_model, history = train_hierarchical_triplet_loss(\n",
    "    tree_data=cluster_data,           \n",
    "    metadata_df=metadata_df,          \n",
    "    model=model,                       # Your SciBERT model\n",
    "    tokenizer=tokenizer,               # Your tokenizer\n",
    "    device=device,                     # Your device\n",
    "    batch_size=16,                     # Adjust based on GPU memory\n",
    "    epochs=3,                          # Number of training epochs\n",
    "    lr=2e-5,                           # Learning rate\n",
    "    margin=0.5,                        # Triplet loss margin\n",
    "    samples_per_leaf=3,                # Triplets per leaf (3 = ~43K triplets)\n",
    "    pooling='cls',                     # 'cls' or 'mean' pooling\n",
    "    train_split=0.9                    # 90% train, 10% validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce2a839-0219-4622-979b-4f74ece8e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_path, device):\n",
    "    \"\"\"Load fine-tuned model and tokenizer\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "    model = AutoModel.from_pretrained(str(model_path)).to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9862f41b-8f0f-4079-986e-bd2bcaf281d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = BASE_DIR / \"cat\" / \"finetuned_hierarchical_scibert\"\n",
    "finetuned_model, tokenizer = load_finetuned_model(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece68657-9947-4b28-a534-004370dfabb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing fine-tuned embeddings...\n",
      "============================================================\n",
      "\n",
      "Sample embeddings shape: (5, 768)\n",
      "First embedding (first 10 dims): [ 0.04233284 -0.0128632   0.04243178  0.00858713  0.00065648 -0.05125532\n",
      " -0.01658175  0.00916764  0.01314754  0.04207835]\n",
      "\n",
      "Pairwise cosine similarities:\n",
      "[[1.         0.8883563  0.89009774 0.3949908  0.32908183]\n",
      " [0.8883563  0.99999994 0.9589499  0.13949977 0.07365996]\n",
      " [0.89009774 0.9589499  0.9999999  0.21137744 0.14096658]\n",
      " [0.3949908  0.13949977 0.21137744 1.         0.98878694]\n",
      " [0.32908183 0.07365996 0.14096658 0.98878694 1.0000002 ]]\n",
      "\n",
      "✓ All done! Your model is now fine-tuned for hierarchical structure.\n"
     ]
    }
   ],
   "source": [
    "# Function to compute embeddings with the fine-tuned model\n",
    "def compute_finetuned_embeddings(texts, model, tokenizer, device):\n",
    "    \"\"\"Compute embeddings using fine-tuned model\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use CLS token (consistent with training)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        # Normalize (consistent with training)\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# Test on a few samples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing fine-tuned embeddings...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a few sample documents\n",
    "sample_ids = metadata_df['id'].head(5).values\n",
    "sample_texts = []\n",
    "for doc_id in sample_ids:\n",
    "    row = metadata_df[metadata_df['id'] == doc_id].iloc[0]\n",
    "    text = f\"{row['title']} {row['abstract']}\"\n",
    "    sample_texts.append(text)\n",
    "\n",
    "# Compute embeddings\n",
    "finetuned_embeddings = compute_finetuned_embeddings(\n",
    "    sample_texts,\n",
    "    finetuned_model,\n",
    "    tokenizer,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"\\nSample embeddings shape: {finetuned_embeddings.shape}\")\n",
    "print(f\"First embedding (first 10 dims): {finetuned_embeddings[0][:10]}\")\n",
    "\n",
    "# Compare similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(finetuned_embeddings)\n",
    "\n",
    "print(\"\\nPairwise cosine similarities:\")\n",
    "print(similarities)\n",
    "\n",
    "print(\"\\n✓ All done! Your model is now fine-tuned for hierarchical structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "451c3858-10f5-484c-8be6-7e3f42caf5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total leaves: 14384\n",
      "Test set size: 1438 leaves\n"
     ]
    }
   ],
   "source": [
    "# Rebuild the tree\n",
    "tree = HierarchicalTree(cluster_data)\n",
    "\n",
    "# Get all leaf names\n",
    "all_leaf_names = list(tree.leaves.keys())\n",
    "print(f\"Total leaves: {len(all_leaf_names)}\")\n",
    "\n",
    "# Determine which leaves were in validation set during training\n",
    "# If you want to use a fresh test set, split here:\n",
    "np.random.seed(42)\n",
    "n_test = int(0.1 * len(all_leaf_names))  # 10% test set\n",
    "test_leaf_indices = np.random.choice(len(all_leaf_names), n_test, replace=False)\n",
    "test_leaves = [all_leaf_names[i] for i in test_leaf_indices]\n",
    "\n",
    "print(f\"Test set size: {len(test_leaves)} leaves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4843dd-23eb-4f36-9d67-68b5a300cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing embeddings for all leaves...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f2a34fc8fa4e4db557d1430a8ff68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing texts:   0%|          | 0/14384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cf54a44aa04a5692aab8764b5e9086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings for 14384 leaves\n"
     ]
    }
   ],
   "source": [
    "def compute_all_embeddings(model, tokenizer, metadata_df, tree, device, batch_size=32):\n",
    "    \"\"\"Compute embeddings for all leaves in the tree\"\"\"\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    leaf_names = list(tree.leaves.keys())\n",
    "    texts = []\n",
    "    valid_names = []\n",
    "\n",
    "    # Prepare texts\n",
    "    for leaf_name in tqdm(leaf_names, desc=\"Preparing texts\"):\n",
    "        try:\n",
    "            row = metadata_df[metadata_df['id'] == int(leaf_name)].iloc[0]\n",
    "            title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "            abstract = str(row['abstract']) if pd.notna(row['abstract']) else \"\"\n",
    "            text = f\"{title} {abstract}\".strip()\n",
    "            texts.append(text)\n",
    "            valid_names.append(leaf_name)\n",
    "        except (KeyError, IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "    # Compute embeddings in batches\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_names = valid_names[i:i+batch_size]\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            # Use CLS token\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            # Normalize\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "            for name, emb in zip(batch_names, embeddings):\n",
    "                embeddings_dict[name] = emb\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "print(\"\\nComputing embeddings for all leaves...\")\n",
    "embeddings_dict = compute_all_embeddings(\n",
    "    finetuned_model,\n",
    "    tokenizer,\n",
    "    metadata_df,\n",
    "    tree,\n",
    "    device,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Computed embeddings for {len(embeddings_dict)} leaves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "658b5f2a-2ad5-46f7-89dc-28d531410ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NETWORK LINK PREDICTION EVALUATION - PASTE INTO NOTEBOOK CELL\n",
    "# ============================================================================\n",
    "\n",
    "from network_link_prediction import (\n",
    "    evaluate_network_link_prediction,\n",
    "    plot_link_prediction_results,\n",
    "    get_node_degree\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96e38409-6164-49f0-ae5e-eed1c7b42f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for 14384 nodes\n",
      "Test set: 1438 nodes\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Path to your edgelist\n",
    "edgelist_path = DATA_DIR / \"network\" / \"oc_mini_edgelist.csv\"\n",
    "\n",
    "# You should already have:\n",
    "# - embeddings_dict: Dict mapping node_id (as string) -> embedding vector\n",
    "# - test_leaves: List of test node IDs (as strings)\n",
    "\n",
    "# If not, compute embeddings first:\n",
    "# embeddings_dict = compute_all_embeddings(finetuned_model, tokenizer, metadata_df, tree, device)\n",
    "\n",
    "print(f\"Embeddings for {len(embeddings_dict)} nodes\")\n",
    "print(f\"Test set: {len(test_leaves)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a58a31-6d2e-4588-ada5-d09765e11718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NETWORK LINK PREDICTION EVALUATION\n",
      "============================================================\n",
      "\n",
      "Loading edgelist from: /home/vikramr2/oc_mini/network/oc_mini_edgelist.csv\n",
      "Loaded 111873 edges\n",
      "Network has 14384 unique nodes\n",
      "Test nodes with embeddings and edges: 1438\n",
      "\n",
      "Evaluating link prediction with top-k retrieval...\n",
      "Test nodes: 1438\n",
      "K values: [5, 10, 20, 50, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacd1006a00a457e98b0dc84fe6d6e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating nodes:   0%|          | 0/1438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating link prediction with AUC metrics...\n",
      "Test nodes: 1438\n",
      "Negative samples per positive: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3f4f7561594cb7983c61c3c722f1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling edges:   0%|          | 0/1438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Top-K Metrics:\n",
      "  K=5:\n",
      "    Precision@5: 0.0339\n",
      "    Recall@5: 0.0166\n",
      "    Hit Rate@5: 0.1481\n",
      "  K=10:\n",
      "    Precision@10: 0.0296\n",
      "    Recall@10: 0.0274\n",
      "    Hit Rate@10: 0.2337\n",
      "  K=20:\n",
      "    Precision@20: 0.0241\n",
      "    Recall@20: 0.0428\n",
      "    Hit Rate@20: 0.3352\n",
      "  K=50:\n",
      "    Precision@50: 0.0182\n",
      "    Recall@50: 0.0761\n",
      "    Hit Rate@50: 0.4930\n",
      "  K=100:\n",
      "    Precision@100: 0.0143\n",
      "    Recall@100: 0.1180\n",
      "    Hit Rate@100: 0.6259\n",
      "\n",
      "AUC Metrics:\n",
      "  AUC-ROC: 0.8017\n",
      "  AUC-PR: 0.3007\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: RUN LINK PREDICTION EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "# Evaluate link prediction\n",
    "link_pred_results = evaluate_network_link_prediction(\n",
    "    edgelist_path=str(edgelist_path),\n",
    "    embeddings_dict=embeddings_dict,\n",
    "    test_nodes=test_leaves,\n",
    "    k_values=[5, 10, 20, 50, 100],      # Top-K values to evaluate\n",
    "    compute_auc=True,                    # Compute AUC-ROC and AUC-PR\n",
    "    num_negative_samples=10              # Negative samples per positive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf9d37-0519-4bad-8ca8-23f4d1f3dd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
