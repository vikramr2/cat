{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ba14cd-3862-43a1-ab0c-c00ea0efc594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting python-docx\n",
      "  Using cached python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Using cached lxml-6.0.2-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting typing_extensions>=4.9.0 (from python-docx)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-12.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Using cached triton-3.5.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Using cached python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Using cached matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Using cached scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached torch-2.9.0-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.5.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.4 MB)\n",
      "Using cached numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "Using cached lxml-6.0.2-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "Using cached pillow-12.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, tzdata, typing_extensions, triton, threadpoolctl, sympy, safetensors, regex, PyPDF2, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, lxml, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, python-docx, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, contourpy, tokenizers, scikit-learn, nvidia-cusolver-cu12, matplotlib, transformers, torch, seaborn\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed PyPDF2-3.0.1 contourpy-1.3.3 cycler-0.12.1 filelock-3.20.0 fonttools-4.60.1 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 joblib-1.5.2 kiwisolver-1.4.9 lxml-6.0.2 matplotlib-3.10.7 mpmath-1.3.0 networkx-3.5 numpy-2.3.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 pillow-12.0.0 pyparsing-3.2.5 python-docx-1.2.0 regex-2025.11.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 seaborn-0.13.2 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.0 transformers-4.57.1 triton-3.5.0 typing_extensions-4.15.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers PyPDF2 python-docx matplotlib scikit-learn seaborn torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f8e425-1570-4ea3-a389-eaaa9f043467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS AND DIRECTORY INITIALIZATION\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"input\" / \"clustering_info\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6efac10-4142-46cd-91e5-9921ba5a4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(texts, model, tokenizer, device):\n",
    "    \"\"\"Compute embeddings for a list of texts.\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c14aee5-c96a-445e-ac0f-73042c423d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cluster data: 19705 nodes\n",
      "\n",
      "First few rows:\n",
      "      node  cluster\n",
      "0    45066        5\n",
      "1   989648        0\n",
      "2  1146632        0\n",
      "3  3732252        0\n",
      "4  9488729        5\n",
      "5  9489474        5\n",
      "6  9489060        5\n",
      "7  6382148        5\n",
      "8  6382959        5\n",
      "9  1623959        5\n",
      "\n",
      "Cluster statistics:\n",
      "  - Total unique nodes: 19705\n",
      "  - Total unique clusters: 5\n",
      "\n",
      "Cluster size distribution:\n",
      "  - Mean cluster size: 3941.00\n",
      "  - Median cluster size: 2480\n",
      "  - Largest cluster: 8989 nodes\n",
      "  - Smallest cluster: 2068 nodes\n",
      "\n",
      "Cluster assignments loaded!\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER LOADING\n",
    "\n",
    "cluster_path = INPUT_DIR / \"oc_mini_clusters_0.001.csv\"\n",
    "metadata_path = INPUT_DIR / \"oc_mini_node_metadata.csv\"\n",
    "\n",
    "cluster_df = pd.read_csv(cluster_path)\n",
    "\n",
    "# Sanity Check\n",
    "print(f\"Loaded cluster data: {cluster_df.shape[0]} nodes\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(cluster_df.head(10))\n",
    "\n",
    "print(f\"\\nCluster statistics:\")\n",
    "print(f\"  - Total unique nodes: {cluster_df['node'].nunique()}\")\n",
    "print(f\"  - Total unique clusters: {cluster_df['cluster'].nunique()}\")\n",
    "print(f\"\\nCluster size distribution:\")\n",
    "cluster_sizes = cluster_df['cluster'].value_counts()\n",
    "print(f\"  - Mean cluster size: {cluster_sizes.mean():.2f}\")\n",
    "print(f\"  - Median cluster size: {cluster_sizes.median():.0f}\")\n",
    "print(f\"  - Largest cluster: {cluster_sizes.max()} nodes\")\n",
    "print(f\"  - Smallest cluster: {cluster_sizes.min()} nodes\")\n",
    "\n",
    "print(f\"\\nCluster assignments loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b8e5da2-ffeb-42d0-a34a-e594c763b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata: 14442 papers\n",
      "\n",
      "Columns: ['id', 'doi', 'title', 'abstract']\n",
      "\n",
      "First few rows:\n",
      "     id                        doi  \\\n",
      "0   128  10.1101/2021.05.10.443415   \n",
      "1   163  10.1101/2021.05.07.443114   \n",
      "2   200  10.1101/2021.05.11.443555   \n",
      "3   941       10.3390/ijms20020449   \n",
      "4  1141       10.3390/ijms20040865   \n",
      "\n",
      "                                               title  \\\n",
      "0  Improved protein contact prediction using dime...   \n",
      "1  Following the Trail of One Million Genomes: Fo...   \n",
      "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...   \n",
      "3  Bactericidal and Cytotoxic Properties of Silve...   \n",
      "4  Silver Nanoparticles: Synthesis and Applicatio...   \n",
      "\n",
      "                                            abstract  \n",
      "0  AbstractDeep residual learning has shown great...  \n",
      "1  AbstractSevere acute respiratory syndrome coro...  \n",
      "2  Molnupiravir is an orally available antiviral ...  \n",
      "3  Silver nanoparticles (AgNPs) can be synthesize...  \n",
      "4  Over the past few decades, metal nanoparticles...  \n",
      "\n",
      "Missing data:\n",
      "id          0\n",
      "doi         0\n",
      "title       0\n",
      "abstract    0\n",
      "dtype: int64\n",
      "\n",
      "Merged dataset: 14442 papers with cluster assignments\n",
      "  - Papers with clusters: 14442\n",
      "  - Papers without clusters: 0\n",
      "\n",
      "Final Merged Table for Fine-Tuning\n",
      "     id                        doi  \\\n",
      "0   128  10.1101/2021.05.10.443415   \n",
      "1   163  10.1101/2021.05.07.443114   \n",
      "2   200  10.1101/2021.05.11.443555   \n",
      "3   941       10.3390/ijms20020449   \n",
      "4  1141       10.3390/ijms20040865   \n",
      "\n",
      "                                               title  \\\n",
      "0  Improved protein contact prediction using dime...   \n",
      "1  Following the Trail of One Million Genomes: Fo...   \n",
      "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...   \n",
      "3  Bactericidal and Cytotoxic Properties of Silve...   \n",
      "4  Silver Nanoparticles: Synthesis and Applicatio...   \n",
      "\n",
      "                                            abstract  node  cluster  \\\n",
      "0  AbstractDeep residual learning has shown great...   128       58   \n",
      "1  AbstractSevere acute respiratory syndrome coro...   163        0   \n",
      "2  Molnupiravir is an orally available antiviral ...   200        0   \n",
      "3  Silver nanoparticles (AgNPs) can be synthesize...   941       40   \n",
      "4  Over the past few decades, metal nanoparticles...  1141       40   \n",
      "\n",
      "                                                text  \n",
      "0  Improved protein contact prediction using dime...  \n",
      "1  Following the Trail of One Million Genomes: Fo...  \n",
      "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...  \n",
      "3  Bactericidal and Cytotoxic Properties of Silve...  \n",
      "4  Silver Nanoparticles: Synthesis and Applicatio...  \n"
     ]
    }
   ],
   "source": [
    "# LINKING NODES TO ACTUAL PAPERS\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "print(f\"Loaded metadata: {metadata_df.shape[0]} papers\")\n",
    "print(f\"\\nColumns: {list(metadata_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(metadata_df.head())\n",
    "\n",
    "# Missing data sanity check\n",
    "print(f\"\\nMissing data:\")\n",
    "print(metadata_df.isnull().sum())\n",
    "\n",
    "# Merge to get clustering\n",
    "papers_df = metadata_df.merge(\n",
    "    cluster_df, \n",
    "    left_on='id',\n",
    "    right_on='node', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged dataset: {papers_df.shape[0]} papers with cluster assignments\")\n",
    "print(f\"  - Papers with clusters: {papers_df.shape[0]}\")\n",
    "print(f\"  - Papers without clusters: {metadata_df.shape[0] - papers_df.shape[0]}\")\n",
    "\n",
    "\n",
    "# Combine title and abstract for later embeddings, can switch to full papers later if we want\n",
    "papers_df['text'] = papers_df['title'].fillna('') + ' ' + papers_df['abstract'].fillna('')\n",
    "\n",
    "print(f\"\\nFinal Merged Table for Fine-Tuning\")\n",
    "print(papers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e61cb6-d006-49b4-909c-81126eb06608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating triplets from 5 clusters...\n",
      "Filtered out 0 single-paper clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating triplets:   2%|▏         | 489/28884 [00:10<10:03, 47.06it/s]"
     ]
    }
   ],
   "source": [
    "# TRIPLET GENERATION FROM CLUSTERS\n",
    "def create_triplets_from_dataframe(papers_df, n_triplets=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate triplets (anchor, positive, negative) from clustered papers.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: DataFrame with columns ['id', 'text', 'cluster']\n",
    "        n_triplets: number of triplets to generate\n",
    "        seed: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        List of (anchor_text, positive_text, negative_text)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Group papers by cluster\n",
    "    cluster_to_papers = papers_df.groupby('cluster')['text'].apply(list).to_dict()\n",
    "    cluster_ids = list(cluster_to_papers.keys())\n",
    "    \n",
    "    # Filter out clusters with only 1 paper because there can be no pairing then\n",
    "    cluster_ids = [cid for cid in cluster_ids if len(cluster_to_papers[cid]) >= 2]\n",
    "    \n",
    "    print(f\"Creating triplets from {len(cluster_ids)} clusters...\")\n",
    "    print(f\"Filtered out {len(cluster_to_papers) - len(cluster_ids)} single-paper clusters\")\n",
    "    \n",
    "    triplets = []\n",
    "    failed_attempts = 0\n",
    "    max_attempts = n_triplets * 3  # Prevent infinite loops\n",
    "    \n",
    "    with tqdm(total=n_triplets, desc=\"Generating triplets\") as pbar:\n",
    "        while len(triplets) < n_triplets and failed_attempts < max_attempts:\n",
    "            \n",
    "            # Random Anchor Cluster\n",
    "            anchor_cluster = np.random.choice(cluster_ids)\n",
    "            \n",
    "            # Select anchor and positive from same cluster\n",
    "            anchor_text, positive_text = np.random.choice(\n",
    "                cluster_to_papers[anchor_cluster], \n",
    "                size=2, \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            # Select negative from different cluster\n",
    "            negative_cluster = np.random.choice(\n",
    "                [c for c in cluster_ids if c != anchor_cluster]\n",
    "            )\n",
    "            negative_text = np.random.choice(cluster_to_papers[negative_cluster])\n",
    "            \n",
    "            triplets.append((anchor_text, positive_text, negative_text))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"Generated {len(triplets)} triplets\")\n",
    "    return triplets\n",
    "\n",
    "# Generate triplets\n",
    "triplets = create_triplets_from_dataframe(papers_df, n_triplets = len(papers_df) * 2)\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample triplet:\")\n",
    "print(f\"ANCHOR: {triplets[0][0][:200]}...\")\n",
    "print(f\"POSITIVE: {triplets[0][1][:200]}...\")\n",
    "print(f\"NEGATIVE: {triplets[0][2][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6498226-c948-48d9-b9d6-357e6364153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS AND MODEL LOADING\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU memory\n",
    "LEARNING_RATE = 2e-5\n",
    "MARGIN = 1.0  # Triplet loss margin\n",
    "SAVE_STEPS = 500  # Save model every N steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfc947c-be37-40b3-a51a-5ddaa06c4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIPLET LOSS TRAINING FUNCTION (WITH RUN-SPECIFIC DIRECTORIES)\n",
    "from datetime import datetime\n",
    "\n",
    "def train_with_triplet_loss(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    triplets, \n",
    "    device,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    margin=MARGIN\n",
    "):\n",
    "    \"\"\"Fine-tune model using triplet loss.\"\"\"\n",
    "    \n",
    "    # Create unique run directory with timestamp\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = OUTPUT_DIR / \"training_runs\" / f\"run_{timestamp}\"\n",
    "    checkpoint_dir = run_dir / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Training run directory: {run_dir}\")\n",
    "    print(f\"Checkpoints will be saved to: {checkpoint_dir}\\n\")\n",
    "    \n",
    "    # Save training configuration\n",
    "    config = {\n",
    "        'timestamp': timestamp,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'margin': margin,\n",
    "        'n_triplets': len(triplets)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(run_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = TripletMarginLoss(margin=margin, p=2)\n",
    "    \n",
    "    losses = []\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Shuffle triplets each epoch\n",
    "        shuffled_triplets = triplets.copy()\n",
    "        np.random.shuffle(shuffled_triplets)\n",
    "        \n",
    "        # Process in batches\n",
    "        progress_bar = tqdm(\n",
    "            range(0, len(shuffled_triplets), batch_size), \n",
    "            desc=f\"Epoch {epoch+1}/{epochs}\"\n",
    "        )\n",
    "        \n",
    "        for i in progress_bar:\n",
    "            batch = shuffled_triplets[i:i+batch_size]\n",
    "            \n",
    "            # Separate anchor, positive, negative\n",
    "            anchors = [t[0] for t in batch]\n",
    "            positives = [t[1] for t in batch]\n",
    "            negatives = [t[2] for t in batch]\n",
    "            \n",
    "            # Tokenize\n",
    "            anchor_inputs = tokenizer(\n",
    "                anchors, padding=True, truncation=True, \n",
    "                max_length=512, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            positive_inputs = tokenizer(\n",
    "                positives, padding=True, truncation=True,\n",
    "                max_length=512, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            negative_inputs = tokenizer(\n",
    "                negatives, padding=True, truncation=True,\n",
    "                max_length=512, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            anchor_outputs = model(**anchor_inputs)\n",
    "            positive_outputs = model(**positive_inputs)\n",
    "            negative_outputs = model(**negative_inputs)\n",
    "            \n",
    "            # Extract [CLS] embeddings\n",
    "            anchor_emb = anchor_outputs.last_hidden_state[:, 0, :]\n",
    "            positive_emb = positive_outputs.last_hidden_state[:, 0, :]\n",
    "            negative_emb = negative_outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            # Compute triplet loss\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            step += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if step % SAVE_STEPS == 0:\n",
    "                checkpoint_path = checkpoint_dir / f\"checkpoint_step_{step}.pt\"\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "                print(f\"\\nCheckpoint saved: {checkpoint_path.name}\")\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_checkpoint_path = checkpoint_dir / f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, epoch_checkpoint_path)\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = run_dir / \"final_model.pt\"\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    \n",
    "    # Save training losses\n",
    "    loss_data = {\n",
    "        'losses': losses,\n",
    "        'epochs': list(range(1, len(losses) + 1))\n",
    "    }\n",
    "    with open(run_dir / \"training_losses.json\", 'w') as f:\n",
    "        json.dump(loss_data, f, indent=2)\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(losses) + 1), losses, marker='o', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title(f'Triplet Loss Training Progress - {timestamp}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(run_dir / \"training_loss.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"All outputs saved to: {run_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, losses, run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2738026b-a04b-4611-894a-2c478da848b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading MedCPT model...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading MedCPT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Article-Encoder\")\n",
    "model = AutoModel.from_pretrained(\"ncbi/MedCPT-Article-Encoder\").to(device)\n",
    "model.train()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61f50f-9a3c-474d-9a6c-600d4ad8d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning with triplet loss...\n",
      "\n",
      "Training run directory: /home/ajgrama2/data/output/training_runs/run_20251106_193924\n",
      "Checkpoints will be saved to: /home/ajgrama2/data/output/training_runs/run_20251106_193924/checkpoints\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  10%|▉         | 89/903 [02:33<23:11,  1.71s/it, loss=0.0329]"
     ]
    }
   ],
   "source": [
    "# RUN TRAINING\n",
    "print(\"Starting fine-tuning with triplet loss...\\n\")\n",
    "\n",
    "finetuned_model, training_losses, run_dir = train_with_triplet_loss(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    triplets, \n",
    "    device,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    margin=MARGIN\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete! Model is ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4724fd97-a981-40e3-a9c5-182dc5b37caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Status:\n",
      "Allocated: 0.44 GB\n",
      "Reserved: 0.49 GB\n",
      "Max allocated: 0.44 GB\n"
     ]
    }
   ],
   "source": [
    "# EMERGENCY: Clear GPU memory\n",
    "import gc\n",
    "\n",
    "# Clear any existing tensors\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check what's using memory\n",
    "print(\"GPU Memory Status:\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "print(f\"Max allocated: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170deed-6657-431c-b089-b77add574013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
