{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba14cd-3862-43a1-ab0c-c00ea0efc594",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers PyPDF2 python-docx matplotlib scikit-learn seaborn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f8e425-1570-4ea3-a389-eaaa9f043467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND DIRECTORY INITIALIZATION\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"input\" / \"clustering_info\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6efac10-4142-46cd-91e5-9921ba5a4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(texts, model, tokenizer, device):\n",
    "    \"\"\"Compute embeddings for a list of texts.\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c14aee5-c96a-445e-ac0f-73042c423d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cluster data: 19705 nodes\n",
      "\n",
      "First few rows:\n",
      "      node  cluster\n",
      "0    45066        5\n",
      "1   989648        0\n",
      "2  1146632        0\n",
      "3  3732252        0\n",
      "4  9488729        5\n",
      "5  9489474        5\n",
      "6  9489060        5\n",
      "7  6382148        5\n",
      "8  6382959        5\n",
      "9  1623959        5\n",
      "\n",
      "Cluster statistics:\n",
      "  - Total unique nodes: 19705\n",
      "  - Total unique clusters: 5\n",
      "\n",
      "Cluster size distribution:\n",
      "  - Mean cluster size: 3941.00\n",
      "  - Median cluster size: 2480\n",
      "  - Largest cluster: 8989 nodes\n",
      "  - Smallest cluster: 2068 nodes\n",
      "\n",
      "Cluster assignments loaded!\n"
     ]
    }
   ],
   "source": [
    "# CLUSTER LOADING\n",
    "\n",
    "cluster_path = INPUT_DIR / \"oc_mini_clusters_0.001.csv\"\n",
    "metadata_path = INPUT_DIR / \"oc_mini_node_metadata.csv\"\n",
    "\n",
    "cluster_df = pd.read_csv(cluster_path)\n",
    "\n",
    "# Sanity Check\n",
    "print(f\"Loaded cluster data: {cluster_df.shape[0]} nodes\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(cluster_df.head(10))\n",
    "\n",
    "print(f\"\\nCluster statistics:\")\n",
    "print(f\"  - Total unique nodes: {cluster_df['node'].nunique()}\")\n",
    "print(f\"  - Total unique clusters: {cluster_df['cluster'].nunique()}\")\n",
    "print(f\"\\nCluster size distribution:\")\n",
    "cluster_sizes = cluster_df['cluster'].value_counts()\n",
    "print(f\"  - Mean cluster size: {cluster_sizes.mean():.2f}\")\n",
    "print(f\"  - Median cluster size: {cluster_sizes.median():.0f}\")\n",
    "print(f\"  - Largest cluster: {cluster_sizes.max()} nodes\")\n",
    "print(f\"  - Smallest cluster: {cluster_sizes.min()} nodes\")\n",
    "\n",
    "print(f\"\\nCluster assignments loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8e5da2-ffeb-42d0-a34a-e594c763b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata: 14442 papers\n",
      "\n",
      "Columns: ['id', 'doi', 'title', 'abstract']\n",
      "\n",
      "First few rows:\n",
      "     id                        doi  \\\n",
      "0   128  10.1101/2021.05.10.443415   \n",
      "1   163  10.1101/2021.05.07.443114   \n",
      "2   200  10.1101/2021.05.11.443555   \n",
      "3   941       10.3390/ijms20020449   \n",
      "4  1141       10.3390/ijms20040865   \n",
      "\n",
      "                                               title  \\\n",
      "0  Improved protein contact prediction using dime...   \n",
      "1  Following the Trail of One Million Genomes: Fo...   \n",
      "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...   \n",
      "3  Bactericidal and Cytotoxic Properties of Silve...   \n",
      "4  Silver Nanoparticles: Synthesis and Applicatio...   \n",
      "\n",
      "                                            abstract  \n",
      "0  AbstractDeep residual learning has shown great...  \n",
      "1  AbstractSevere acute respiratory syndrome coro...  \n",
      "2  Molnupiravir is an orally available antiviral ...  \n",
      "3  Silver nanoparticles (AgNPs) can be synthesize...  \n",
      "4  Over the past few decades, metal nanoparticles...  \n",
      "\n",
      "Missing data:\n",
      "id          0\n",
      "doi         0\n",
      "title       0\n",
      "abstract    0\n",
      "dtype: int64\n",
      "\n",
      "Merged dataset: 14442 papers with cluster assignments\n",
      "  - Papers with clusters: 14442\n",
      "  - Papers without clusters: 0\n",
      "\n",
      "Final Merged Table for Fine-Tuning\n",
      "     id                        doi  \\\n",
      "0   128  10.1101/2021.05.10.443415   \n",
      "1   163  10.1101/2021.05.07.443114   \n",
      "2   200  10.1101/2021.05.11.443555   \n",
      "3   941       10.3390/ijms20020449   \n",
      "4  1141       10.3390/ijms20040865   \n",
      "\n",
      "                                               title  \\\n",
      "0  Improved protein contact prediction using dime...   \n",
      "1  Following the Trail of One Million Genomes: Fo...   \n",
      "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...   \n",
      "3  Bactericidal and Cytotoxic Properties of Silve...   \n",
      "4  Silver Nanoparticles: Synthesis and Applicatio...   \n",
      "\n",
      "                                            abstract  node  cluster  \\\n",
      "0  AbstractDeep residual learning has shown great...   128       58   \n",
      "1  AbstractSevere acute respiratory syndrome coro...   163        0   \n",
      "2  Molnupiravir is an orally available antiviral ...   200        0   \n",
      "3  Silver nanoparticles (AgNPs) can be synthesize...   941       40   \n",
      "4  Over the past few decades, metal nanoparticles...  1141       40   \n",
      "\n",
      "                                                text  \n",
      "0  Improved protein contact prediction using dime...  \n",
      "1  Following the Trail of One Million Genomes: Fo...  \n",
      "2  Mechanism of molnupiravir-induced SARS-CoV-2 m...  \n",
      "3  Bactericidal and Cytotoxic Properties of Silve...  \n",
      "4  Silver Nanoparticles: Synthesis and Applicatio...  \n"
     ]
    }
   ],
   "source": [
    "# LINKING NODES TO ACTUAL PAPERS\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "print(f\"Loaded metadata: {metadata_df.shape[0]} papers\")\n",
    "print(f\"\\nColumns: {list(metadata_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(metadata_df.head())\n",
    "\n",
    "# Missing data sanity check\n",
    "print(f\"\\nMissing data:\")\n",
    "print(metadata_df.isnull().sum())\n",
    "\n",
    "# Merge to get clustering\n",
    "papers_df = metadata_df.merge(\n",
    "    cluster_df, \n",
    "    left_on='id',\n",
    "    right_on='node', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged dataset: {papers_df.shape[0]} papers with cluster assignments\")\n",
    "print(f\"  - Papers with clusters: {papers_df.shape[0]}\")\n",
    "print(f\"  - Papers without clusters: {metadata_df.shape[0] - papers_df.shape[0]}\")\n",
    "\n",
    "\n",
    "# Combine title and abstract for later embeddings, can switch to full papers later if we want\n",
    "papers_df['text'] = papers_df['title'].fillna('') + ' ' + papers_df['abstract'].fillna('')\n",
    "\n",
    "print(f\"\\nFinal Merged Table for Fine-Tuning\")\n",
    "print(papers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e61cb6-d006-49b4-909c-81126eb06608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating triplets from 5 clusters...\n",
      "Filtered out 0 single-paper clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating triplets: 100%|██████████| 28884/28884 [10:08<00:00, 47.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 28884 triplets\n",
      "\n",
      "Example triplet:\n",
      "ANCHOR: Synthesis of a Zinc Oxide Nanoflower Photocatalyst from Sea Buckthorn Fruit for Degradation of Industrial Dyes in Wastewater Treatment Green synthesis of ZnO nanoparticles has attracted research atten...\n",
      "POSITIVE: Leonotis nepetifolia Flower Bud Extract Mediated Green Synthesis of Silver Nanoparticles, Their Characterization, and In Vitro Evaluation of Biological Applications Biosynthesis of silver nanoparticle...\n",
      "NEGATIVE: CysPresso: A classification model utilizing deep learning protein representations to predict recombinant expression of cysteine-dense peptides AbstractBackground:Cysteine-dense peptides (CDPs) are an ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRIPLET GENERATION FROM CLUSTERS\n",
    "def create_triplets_from_dataframe(papers_df, n_triplets=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate triplets (anchor, positive, negative) from clustered papers.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: DataFrame with columns ['id', 'text', 'cluster']\n",
    "        n_triplets: number of triplets to generate\n",
    "        seed: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        List of (anchor_text, positive_text, negative_text)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Group papers by cluster\n",
    "    cluster_to_papers = papers_df.groupby('cluster')['text'].apply(list).to_dict()\n",
    "    cluster_ids = list(cluster_to_papers.keys())\n",
    "    \n",
    "    # Filter out clusters with only 1 paper because there can be no pairing then\n",
    "    cluster_ids = [cid for cid in cluster_ids if len(cluster_to_papers[cid]) >= 2]\n",
    "    \n",
    "    print(f\"Creating triplets from {len(cluster_ids)} clusters...\")\n",
    "    print(f\"Filtered out {len(cluster_to_papers) - len(cluster_ids)} single-paper clusters\")\n",
    "    \n",
    "    triplets = []\n",
    "    failed_attempts = 0\n",
    "    max_attempts = n_triplets * 3  # Prevent infinite loops\n",
    "    \n",
    "    with tqdm(total=n_triplets, desc=\"Generating triplets\") as pbar:\n",
    "        while len(triplets) < n_triplets and failed_attempts < max_attempts:\n",
    "            \n",
    "            # Random Anchor Cluster\n",
    "            anchor_cluster = np.random.choice(cluster_ids)\n",
    "            \n",
    "            # Select anchor and positive from same cluster\n",
    "            anchor_text, positive_text = np.random.choice(\n",
    "                cluster_to_papers[anchor_cluster], \n",
    "                size=2, \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            # Select negative from different cluster\n",
    "            negative_cluster = np.random.choice(\n",
    "                [c for c in cluster_ids if c != anchor_cluster]\n",
    "            )\n",
    "            negative_text = np.random.choice(cluster_to_papers[negative_cluster])\n",
    "            \n",
    "            triplets.append((anchor_text, positive_text, negative_text))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"Generated {len(triplets)} triplets\")\n",
    "    return triplets\n",
    "\n",
    "# Generate triplets\n",
    "triplets = create_triplets_from_dataframe(papers_df, n_triplets = len(papers_df) * 2)\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample triplet:\")\n",
    "print(f\"ANCHOR: {triplets[0][0][:200]}...\")\n",
    "print(f\"POSITIVE: {triplets[0][1][:200]}...\")\n",
    "print(f\"NEGATIVE: {triplets[0][2][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6498226-c948-48d9-b9d6-357e6364153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS AND MODEL LOADING\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU memory\n",
    "LEARNING_RATE = 2e-5\n",
    "MARGIN = 1.0  # Triplet loss margin\n",
    "SAVE_STEPS = 500  # Save model every N steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfc947c-be37-40b3-a51a-5ddaa06c4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIPLET LOSS TRAINING FUNCTION (WITH RUN-SPECIFIC DIRECTORIES)\n",
    "from datetime import datetime\n",
    "\n",
    "def train_with_triplet_loss(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    triplets, \n",
    "    device,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    margin=MARGIN\n",
    "):\n",
    "    \"\"\"Fine-tune model using triplet loss.\"\"\"\n",
    "    \n",
    "    # Create unique run directory with timestamp\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = OUTPUT_DIR / \"training_runs\" / f\"run_{timestamp}\"\n",
    "    checkpoint_dir = run_dir / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Training run directory: {run_dir}\")\n",
    "    print(f\"Checkpoints will be saved to: {checkpoint_dir}\\n\")\n",
    "    \n",
    "    # Save training configuration\n",
    "    config = {\n",
    "        'timestamp': timestamp,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'margin': margin,\n",
    "        'n_triplets': len(triplets)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(run_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = TripletMarginLoss(margin=margin, p=2)\n",
    "    \n",
    "    losses = []\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Shuffle triplets each epoch\n",
    "        shuffled_triplets = triplets.copy()\n",
    "        np.random.shuffle(shuffled_triplets)\n",
    "        \n",
    "        # Process in batches\n",
    "        progress_bar = tqdm(\n",
    "            range(0, len(shuffled_triplets), batch_size), \n",
    "            desc=f\"Epoch {epoch+1}/{epochs}\"\n",
    "        )\n",
    "        \n",
    "        for i in progress_bar:\n",
    "            batch = shuffled_triplets[i:i+batch_size]\n",
    "            \n",
    "            # Separate anchor, positive, negative\n",
    "            anchors = [t[0] for t in batch]\n",
    "            positives = [t[1] for t in batch]\n",
    "            negatives = [t[2] for t in batch]\n",
    "            \n",
    "            # Tokenize\n",
    "            anchor_inputs = tokenizer(\n",
    "                anchors, padding=True, truncation=True, \n",
    "                max_length=512, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            positive_inputs = tokenizer(\n",
    "                positives, padding=True, truncation=True,\n",
    "                max_length=512, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            negative_inputs = tokenizer(\n",
    "                negatives, padding=True, truncation=True,\n",
    "                max_length=512, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            anchor_outputs = model(**anchor_inputs)\n",
    "            positive_outputs = model(**positive_inputs)\n",
    "            negative_outputs = model(**negative_inputs)\n",
    "            \n",
    "            # Extract [CLS] embeddings\n",
    "            anchor_emb = anchor_outputs.last_hidden_state[:, 0, :]\n",
    "            positive_emb = positive_outputs.last_hidden_state[:, 0, :]\n",
    "            negative_emb = negative_outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            # Compute triplet loss\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            step += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if step % SAVE_STEPS == 0:\n",
    "                checkpoint_path = checkpoint_dir / f\"checkpoint_step_{step}.pt\"\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "                print(f\"\\nCheckpoint saved: {checkpoint_path.name}\")\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_checkpoint_path = checkpoint_dir / f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, epoch_checkpoint_path)\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = run_dir / \"final_model.pt\"\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    \n",
    "    # Save training losses\n",
    "    loss_data = {\n",
    "        'losses': losses,\n",
    "        'epochs': list(range(1, len(losses) + 1))\n",
    "    }\n",
    "    with open(run_dir / \"training_losses.json\", 'w') as f:\n",
    "        json.dump(loss_data, f, indent=2)\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(losses) + 1), losses, marker='o', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title(f'Triplet Loss Training Progress - {timestamp}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(run_dir / \"training_loss.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"All outputs saved to: {run_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, losses, run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2738026b-a04b-4611-894a-2c478da848b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading MedCPT model...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading MedCPT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Article-Encoder\")\n",
    "model = AutoModel.from_pretrained(\"ncbi/MedCPT-Article-Encoder\").to(device)\n",
    "model.train()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61f50f-9a3c-474d-9a6c-600d4ad8d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning with triplet loss...\n",
      "\n",
      "Training run directory: /home/ajgrama2/data/output/training_runs/run_20251106_193924\n",
      "Checkpoints will be saved to: /home/ajgrama2/data/output/training_runs/run_20251106_193924/checkpoints\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  10%|▉         | 89/903 [02:33<23:11,  1.71s/it, loss=0.0329]"
     ]
    }
   ],
   "source": [
    "# RUN TRAINING\n",
    "print(\"Starting fine-tuning with triplet loss...\\n\")\n",
    "\n",
    "finetuned_model, training_losses, run_dir = train_with_triplet_loss(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    triplets, \n",
    "    device,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    margin=MARGIN\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete! Model is ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4724fd97-a981-40e3-a9c5-182dc5b37caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Status:\n",
      "Allocated: 0.44 GB\n",
      "Reserved: 0.49 GB\n",
      "Max allocated: 0.44 GB\n"
     ]
    }
   ],
   "source": [
    "# EMERGENCY: Clear GPU memory\n",
    "import gc\n",
    "\n",
    "# Clear any existing tensors\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check what's using memory\n",
    "print(\"GPU Memory Status:\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "print(f\"Max allocated: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170deed-6657-431c-b089-b77add574013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
